{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загрузка библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/vitalii/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/vitalii/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "import gensim.downloader as api\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from razdel import tokenize, sentenize\n",
    "import re\n",
    "import string\n",
    "from rnnmorph.predictor import RNNMorphPredictor\n",
    "import pymorphy2\n",
    "import spacy\n",
    "import pymystem3\n",
    "\"\"\"\n",
    "pyemestem3 работает быстрее при обработке\n",
    "большого текста в моменте, по сравнению с\n",
    "pymorphy2. Но если запускать pymystem3\n",
    "много раз на кусках текста, то работа\n",
    "будет медленной\n",
    "\"\"\"\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# in terminal:\n",
    "# python -m spacy download en_core_web_sm\n",
    "# pip install scipy==1.10.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Примеры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['all', 'work', 'and', 'no', 'play', 'makes', 'jack', 'a', 'dull', 'boy', ',', 'all', 'work', 'and', 'no', 'play'] <class 'list'>\n",
      "['I was going home when whe runs.', 'It was a surprise.']\n"
     ]
    }
   ],
   "source": [
    "data = \"All work and no play makes jack a dull boy, all work and no play\"\n",
    "tokens = word_tokenize(data.lower())\n",
    "print(tokens, type(tokens))\n",
    "print(sent_tokenize(\"I was going home when whe runs. It was a surprise.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Substring(0, 13, 'Кружка-термос'),\n",
       " Substring(14, 16, 'на'),\n",
       " Substring(17, 20, '0.5'),\n",
       " Substring(20, 21, 'л'),\n",
       " Substring(22, 23, '('),\n",
       " Substring(23, 28, '50/64'),\n",
       " Substring(29, 32, 'см²'),\n",
       " Substring(32, 33, ','),\n",
       " Substring(34, 37, '516'),\n",
       " Substring(37, 38, ';'),\n",
       " Substring(38, 41, '...'),\n",
       " Substring(41, 42, ')')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# razdel\n",
    "text = \"Кружка-термос на 0.5л (50/64 см², 516;...)\"\n",
    "list(tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['кружка-термос', 'на', '0.5л', '(', '50/64', 'см²', ',', '516', ';', '...', ')']\n"
     ]
    }
   ],
   "source": [
    "# nltk\n",
    "tokens = word_tokenize(text.lower())\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-gramm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('all',), ('work',), ('and',), ('no',), ('play',)]\n",
      "[('all', 'work'), ('work', 'and'), ('and', 'no'), ('no', 'play'), ('play', 'makes')]\n",
      "[('all', 'work', 'and'), ('work', 'and', 'no'), ('and', 'no', 'play'), ('no', 'play', 'makes'), ('play', 'makes', 'jack')]\n"
     ]
    }
   ],
   "source": [
    "data = \"All work and no play makes jack a dull boy, all work and no play\"\n",
    "tokens = word_tokenize(data.lower())\n",
    "unigram = list(nltk.ngrams(tokens, 1))\n",
    "bigram = list(nltk.ngrams(tokens, 2))\n",
    "trigram = list(nltk.ngrams(tokens, 3))\n",
    "print(unigram[:5])\n",
    "print(bigram[:5])\n",
    "print(trigram[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Стоп-слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'то', 'чего', 'иногда', 'где', 'совсем', 'хорошо', 'как', 'чем', 'под', 'три', 'все', 'в', 'если', 'при', 'за', 'себя', 'ей', 'такой', 'на', 'всегда', 'тут', 'ж', 'мой', 'что', 'так', 'этот', 'эту', 'со', 'куда', 'тебя', 'можно', 'его', 'был', 'ну', 'всю', 'после', 'кто', 'им', 'они', 'моя', 'ее', 'есть', 'их', 'бы', 'эти', 'этом', 'даже', 'теперь', 'об', 'но', 'ни', 'нибудь', 'не', 'да', 'было', 'себе', 'ней', 'тем', 'чтобы', 'же', 'до', 'впрочем', 'у', 'быть', 'были', 'более', 'ты', 'свою', 'много', 'этой', 'никогда', 'чуть', 'уже', 'будто', 'тоже', 'всех', 'там', 'раз', 'потом', 'надо', 'хоть', 'сам', 'том', 'здесь', 'вы', 'только', 'уж', 'нет', 'он', 'всего', 'вам', 'без', 'между', 'к', 'разве', 'нельзя', 'по', 'меня', 'для', 'ему', 'нее', 'из', 'конечно', 'потому', 'опять', 'сейчас', 'через', 'во', 'я', 'вот', 'мы', 'чтоб', 'ним', 'больше', 'лучше', 'еще', 'была', 'этого', 'один', 'или', 'другой', 'над', 'может', 'тот', 'ничего', 'какой', 'перед', 'вдруг', 'зачем', 'ли', 'а', 'и', 'того', 'про', 'она', 'наконец', 'от', 'мне', 'о', 'когда', 'какая', 'ведь', 'нас', 'почти', 'два', 'них', 'будет', 'вас', 'тогда', 'него', 'с'}\n",
      "151\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('russian'))\n",
    "print(stop_words)\n",
    "print(len(stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'who', 'do', 'her', 'own', 'are', 'down', 't', \"won't\", 'some', 'was', 'all', 'can', 'does', 'out', 'under', 'ma', 'over', 'through', 'the', 'don', 'too', 'that', 'doing', 'whom', 'll', 'to', 'those', 'i', 'no', 'above', \"weren't\", 'isn', 'themselves', \"you've\", \"wasn't\", 'won', 've', 'him', 'its', 'again', 'in', \"she's\", 'both', 'his', 'but', 'ain', 'himself', 'hadn', 'needn', \"it's\", 'aren', \"shan't\", 'couldn', 'than', 'd', 'where', 'during', 'have', \"should've\", 'until', 'an', 're', 'now', 'it', 'they', 'only', 'shan', 'about', \"you'd\", 'should', 'been', 'when', 'these', 'yourself', 'didn', 'had', 'and', 'is', 'which', 'did', 'yourselves', 'further', 'below', \"wouldn't\", 'or', 'from', 'why', \"you'll\", \"doesn't\", 'myself', 'once', 'weren', 'with', \"needn't\", 'same', \"mustn't\", 'as', 'ourselves', 'itself', 'on', 'your', \"aren't\", 'mightn', 'at', 'their', 'hasn', 'if', 'because', 'doesn', 'we', 'hers', 'not', 'for', 'haven', 'each', 'he', 'you', 'there', 'between', 'ours', 'theirs', \"shouldn't\", 'be', 'me', 'yours', 'will', 'what', 'most', 'other', \"didn't\", \"don't\", 'of', 'very', 'nor', 's', 'so', 'shouldn', 'having', 'few', 'any', 'up', 'how', 'o', 'off', 'just', 'herself', 'am', 'a', 'into', 'while', \"hasn't\", 'such', 'm', 'y', \"haven't\", 'this', 'were', \"that'll\", 'wasn', 'more', 'then', 'has', \"isn't\", \"couldn't\", 'by', 'against', 'mustn', 'my', 'here', \"hadn't\", 'wouldn', 'she', \"mightn't\", 'before', 'being', 'our', 'them', 'after', \"you're\"}\n",
      "179\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "print(stop_words)\n",
    "print(len(stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['work', 'play', 'makes', 'jack', 'dull', 'boy', ',', 'work', 'play']\n"
     ]
    }
   ],
   "source": [
    "print([word for word in tokens if word not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Стемминг и лемматизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_en = [\"game\", \"gaming\", \"gamed\", \"games\", \"compacted\"]\n",
    "words_ru = [\"корова\", \"мальчики\", \"мужчины\", \"столом\", \"убежала\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['game', 'game', 'game', 'game', 'compact']\n",
      "['корова', 'мальчики', 'мужчины', 'столом', 'убежала']\n"
     ]
    }
   ],
   "source": [
    "ps = PorterStemmer()\n",
    "print(list(map(ps.stem, words_en)))\n",
    "print(list(map(ps.stem, words_ru)))  # Старый метод не работает с русским языком"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['game', 'game', 'game', 'game', 'compact']\n",
      "['коров', 'мальчик', 'мужчин', 'стол', 'убежа']\n"
     ]
    }
   ],
   "source": [
    "ss = SnowballStemmer(language=\"english\")\n",
    "print(list(map(ss.stem, words_en)))\n",
    "ss = SnowballStemmer(language=\"russian\")\n",
    "print(list(map(ss.stem, words_ru)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_en = \"\"\"DENNIS: Listen, strange women lying in ponds \n",
    "distributing swords is no basis for a system of\n",
    "government. Supreme executive power derives from a\n",
    "mandate from the masses, not from some farcical aquatic ceremony.\n",
    "\"\"\"\n",
    "\n",
    "text_ru = \"\"\"Не существует научных доказательств в пользу\n",
    "эффективности НЛП, оно призвано псевдонаукой. Систематические\n",
    "обзоры указывают, что НЛП основано на устаревших\n",
    "представлениях об устройстве мозга, несовместимо с современной\n",
    "неврологией и содержит ряд фактических ошибок.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "не существовать научный доказательство в пользу\n",
      "эффективность нлп, оно призвать псевдонаукой. систематические\n",
      "обзор указывают, что нлп основать на устаревших\n",
      "представление о устройство мозга, несовместимый с современной\n",
      "неврология и содержать ряд фактический ошибок.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()\n",
    "pymorphy_results = list(map(lambda x: morph.parse(x), text_ru.split(' ')))\n",
    "print(' '.join([res[0].normal_form for res in pymorphy_results]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Разборы слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parse(word='не', tag=OpencorporaTag('PRCL'), normal_form='не', score=1.0, methods_stack=((DictionaryAnalyzer(), 'не', 22, 0),))]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pymorphy_results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parse(word='научных', tag=OpencorporaTag('ADJF,Qual plur,gent'), normal_form='научный', score=0.774193, methods_stack=((DictionaryAnalyzer(), 'научных', 12, 21),)),\n",
       " Parse(word='научных', tag=OpencorporaTag('ADJF,Qual plur,loct'), normal_form='научный', score=0.209677, methods_stack=((DictionaryAnalyzer(), 'научных', 12, 26),)),\n",
       " Parse(word='научных', tag=OpencorporaTag('ADJF,Qual anim,plur,accs'), normal_form='научный', score=0.016129, methods_stack=((DictionaryAnalyzer(), 'научных', 12, 23),))]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pymorphy_results[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DENNIS : listen , strange woman lie in pond \n",
      " distribute sword be no basis for a system of \n",
      " government . Supreme executive power derive from a \n",
      " mandate from the masse , not from some farcical aquatic ceremony . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "spacy_results = nlp(text_en)\n",
    "print(' '.join([token.lemma_ for token in spacy_results]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задачи"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part-of-speech (определение части речи)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('не', OpencorporaTag('PRCL')),\n",
       " ('существовать', OpencorporaTag('VERB,impf,intr sing,3per,pres,indc')),\n",
       " ('научный', OpencorporaTag('ADJF,Qual plur,gent')),\n",
       " ('доказательство', OpencorporaTag('NOUN,inan,neut plur,gent')),\n",
       " ('в', OpencorporaTag('PREP')),\n",
       " ('пользу\\nэффективность', OpencorporaTag('NOUN,inan,femn sing,gent')),\n",
       " ('нлп,', OpencorporaTag('UNKN')),\n",
       " ('оно', OpencorporaTag('NPRO,neut,3per,Anph sing,nomn')),\n",
       " ('призвать', OpencorporaTag('PRTS,perf,past,pssv neut,sing'))]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1.\n",
    "[(res[0].normal_form, res[0].tag) for res in pymorphy_results[:9]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('DENNIS', 'PROPN'),\n",
       " (':', 'PUNCT'),\n",
       " ('listen', 'VERB'),\n",
       " (',', 'PUNCT'),\n",
       " ('strange', 'ADJ'),\n",
       " ('woman', 'NOUN'),\n",
       " ('lie', 'VERB')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2.\n",
    "[(token.lemma_, token.pos_) for token in spacy_results[:7]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Could not locate class 'Functional'. Make sure custom classes are decorated with `@keras.saving.register_keras_serializable()`. Full object config: {'class_name': 'Functional', 'config': {'name': 'model_1', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': [None, None, 56], 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'grammemes'}, 'name': 'grammemes', 'inbound_nodes': []}, {'class_name': 'Dropout', 'config': {'name': 'dropout_1', 'trainable': True, 'dtype': 'float32', 'rate': 0.2, 'noise_shape': None, 'seed': None}, 'name': 'dropout_1', 'inbound_nodes': [[['grammemes', 0, 0, {}]]]}, {'class_name': 'InputLayer', 'config': {'batch_input_shape': [None, None, 32], 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'chars'}, 'name': 'chars', 'inbound_nodes': []}, {'class_name': 'Dense', 'config': {'name': 'dense_1', 'trainable': True, 'dtype': 'float32', 'units': 30, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'name': 'dense_1', 'inbound_nodes': [[['dropout_1', 0, 0, {}]]]}, {'class_name': 'TimeDistributed', 'config': {'name': 'time_distributed_1', 'trainable': True, 'dtype': 'float32', 'layer': {'class_name': 'Functional', 'config': {'name': 'model_1', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': [None, 32], 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'chars'}, 'name': 'chars', 'inbound_nodes': []}, {'class_name': 'Embedding', 'config': {'name': 'chars_embeddings', 'trainable': True, 'batch_input_shape': [None, None], 'dtype': 'float32', 'input_dim': 101, 'output_dim': 24, 'embeddings_initializer': {'class_name': 'RandomUniform', 'config': {'minval': -0.05, 'maxval': 0.05, 'seed': None}}, 'embeddings_regularizer': None, 'activity_regularizer': None, 'embeddings_constraint': None, 'mask_zero': False, 'input_length': None}, 'name': 'chars_embeddings', 'inbound_nodes': [[['chars', 0, 0, {}]]]}, {'class_name': 'Reshape', 'config': {'name': 'reshape_1', 'trainable': True, 'dtype': 'float32', 'target_shape': [768]}, 'name': 'reshape_1', 'inbound_nodes': [[['chars_embeddings', 0, 0, {}]]]}, {'class_name': 'Dropout', 'config': {'name': 'dropout_2', 'trainable': True, 'dtype': 'float32', 'rate': 0.2, 'noise_shape': None, 'seed': None}, 'name': 'dropout_2', 'inbound_nodes': [[['reshape_1', 0, 0, {}]]]}, {'class_name': 'Dense', 'config': {'name': 'dense_2', 'trainable': True, 'dtype': 'float32', 'units': 500, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'name': 'dense_2', 'inbound_nodes': [[['dropout_2', 0, 0, {}]]]}, {'class_name': 'Dropout', 'config': {'name': 'dropout_3', 'trainable': True, 'dtype': 'float32', 'rate': 0.2, 'noise_shape': None, 'seed': None}, 'name': 'dropout_3', 'inbound_nodes': [[['dense_2', 0, 0, {}]]]}, {'class_name': 'Dense', 'config': {'name': 'dense_3', 'trainable': True, 'dtype': 'float32', 'units': 200, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'name': 'dense_3', 'inbound_nodes': [[['dropout_3', 0, 0, {}]]]}, {'class_name': 'Dropout', 'config': {'name': 'dropout_4', 'trainable': True, 'dtype': 'float32', 'rate': 0.2, 'noise_shape': None, 'seed': None}, 'name': 'dropout_4', 'inbound_nodes': [[['dense_3', 0, 0, {}]]]}], 'input_layers': [['chars', 0, 0]], 'output_layers': [['dropout_4', 0, 0]]}}}, 'name': 'time_distributed_1', 'inbound_nodes': [[['chars', 0, 0, {}]]]}, {'class_name': 'Concatenate', 'config': {'name': 'LSTM_input', 'trainable': True, 'dtype': 'float32', 'axis': -1}, 'name': 'LSTM_input', 'inbound_nodes': [[['dense_1', 0, 0, {}], ['time_distributed_1', 0, 0, {}]]]}, {'class_name': 'Dense', 'config': {'name': 'dense_4', 'trainable': True, 'dtype': 'float32', 'units': 200, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'name': 'dense_4', 'inbound_nodes': [[['LSTM_input', 0, 0, {}]]]}, {'class_name': 'LSTM', 'config': {'name': 'LSTM_1_forward', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': False, 'stateful': False, 'unroll': False, 'time_major': False, 'units': 128, 'activation': 'tanh', 'recurrent_activation': 'hard_sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None}, 'shared_object_id': 25}, 'recurrent_initializer': {'class_name': 'Orthogonal', 'config': {'gain': 1.0, 'seed': None}, 'shared_object_id': 26}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}, 'shared_object_id': 27}, 'unit_forget_bias': True, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.3, 'recurrent_dropout': 0.3, 'implementation': 1}, 'name': 'LSTM_1_forward', 'inbound_nodes': [[['dense_4', 0, 0, {}]]]}, {'class_name': 'ReversedLSTM', 'config': {'name': 'LSTM_1_backward', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': True, 'stateful': False, 'unroll': False, 'time_major': False, 'units': 128, 'activation': 'tanh', 'recurrent_activation': 'hard_sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None}, 'shared_object_id': 30}, 'recurrent_initializer': {'class_name': 'Orthogonal', 'config': {'gain': 1.0, 'seed': None}, 'shared_object_id': 31}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}, 'shared_object_id': 32}, 'unit_forget_bias': True, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.3, 'recurrent_dropout': 0.3, 'implementation': 1}, 'name': 'LSTM_1_backward', 'inbound_nodes': [[['dense_4', 0, 0, {}]]]}, {'class_name': 'Concatenate', 'config': {'name': 'BiLSTM_input', 'trainable': True, 'dtype': 'float32', 'axis': -1}, 'name': 'BiLSTM_input', 'inbound_nodes': [[['LSTM_1_forward', 0, 0, {}], ['LSTM_1_backward', 0, 0, {}]]]}, {'class_name': 'Bidirectional', 'config': {'name': 'bidirectional_1', 'trainable': True, 'dtype': 'float32', 'layer': {'class_name': 'LSTM', 'config': {'name': 'LSTM_0', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': False, 'stateful': False, 'unroll': False, 'time_major': False, 'units': 128, 'activation': 'tanh', 'recurrent_activation': 'hard_sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None}, 'shared_object_id': 36}, 'recurrent_initializer': {'class_name': 'Orthogonal', 'config': {'gain': 1.0, 'seed': None}, 'shared_object_id': 37}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}, 'shared_object_id': 38}, 'unit_forget_bias': True, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.3, 'recurrent_dropout': 0.3, 'implementation': 1}}, 'merge_mode': 'concat'}, 'name': 'bidirectional_1', 'inbound_nodes': [[['BiLSTM_input', 0, 0, {}]]]}, {'class_name': 'TimeDistributed', 'config': {'name': 'time_distributed_2', 'trainable': True, 'dtype': 'float32', 'layer': {'class_name': 'Dense', 'config': {'name': 'dense_5', 'trainable': True, 'dtype': 'float32', 'units': 128, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}}, 'name': 'time_distributed_2', 'inbound_nodes': [[['bidirectional_1', 0, 0, {}]]]}, {'class_name': 'TimeDistributed', 'config': {'name': 'time_distributed_3', 'trainable': True, 'dtype': 'float32', 'layer': {'class_name': 'Dropout', 'config': {'name': 'dropout_2', 'trainable': True, 'dtype': 'float32', 'rate': 0.2, 'noise_shape': None, 'seed': None}}}, 'name': 'time_distributed_3', 'inbound_nodes': [[['time_distributed_2', 0, 0, {}]]]}, {'class_name': 'TimeDistributed', 'config': {'name': 'time_distributed_4', 'trainable': True, 'dtype': 'float32', 'layer': {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_1', 'trainable': True, 'dtype': 'float32', 'axis': [1], 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}}, 'name': 'time_distributed_4', 'inbound_nodes': [[['time_distributed_3', 0, 0, {}]]]}, {'class_name': 'TimeDistributed', 'config': {'name': 'time_distributed_5', 'trainable': True, 'dtype': 'float32', 'layer': {'class_name': 'Activation', 'config': {'name': 'activation_1', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}}, 'name': 'time_distributed_5', 'inbound_nodes': [[['time_distributed_4', 0, 0, {}]]]}, {'class_name': 'Dense', 'config': {'name': 'main_pred', 'trainable': True, 'dtype': 'float32', 'units': 253, 'activation': 'softmax', 'use_bias': True, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'name': 'main_pred', 'inbound_nodes': [[['time_distributed_5', 0, 0, {}]]]}], 'input_layers': [['grammemes', 0, 0], ['chars', 0, 0]], 'output_layers': [['main_pred', 0, 0]]}, 'keras_version': '2.8.0', 'backend': 'tensorflow'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 3.\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m predictor \u001b[38;5;241m=\u001b[39m \u001b[43mRNNMorphPredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mru\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m rnnmorph_result \u001b[38;5;241m=\u001b[39m predictor\u001b[38;5;241m.\u001b[39mpredict(text_ru\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      4\u001b[0m [(token\u001b[38;5;241m.\u001b[39mnormal_form, token\u001b[38;5;241m.\u001b[39mposm, token\u001b[38;5;241m.\u001b[39mtag) \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m rnnmorph_result[:\u001b[38;5;241m7\u001b[39m]]\n",
      "File \u001b[0;32m~/source/dls_2sem_nlp/.venv/lib/python3.10/site-packages/rnnmorph/predictor.py:88\u001b[0m, in \u001b[0;36mRNNMorphPredictor.__init__\u001b[0;34m(self, language, eval_model_config_path, eval_model_weights_path, gram_dict_input, gram_dict_output, word_vocabulary, char_set_path, build_config)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m LSTMMorphoAnalysis(language\u001b[38;5;241m=\u001b[39mlanguage)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mprepare(gram_dict_input, gram_dict_output, word_vocabulary, char_set_path)\n\u001b[0;32m---> 88\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_model_config_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_model_weights_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/source/dls_2sem_nlp/.venv/lib/python3.10/site-packages/rnnmorph/model.py:138\u001b[0m, in \u001b[0;36mLSTMMorphoAnalysis.load_eval\u001b[0;34m(self, config, eval_model_config_path, eval_model_weights_path)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    137\u001b[0m         custom_objects \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReversedLSTM\u001b[39m\u001b[38;5;124m'\u001b[39m: ReversedLSTM}\n\u001b[0;32m--> 138\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval_model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_from_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval_model\u001b[38;5;241m.\u001b[39mload_weights(eval_model_weights_path)\n",
      "File \u001b[0;32m~/source/dls_2sem_nlp/.venv/lib/python3.10/site-packages/keras/src/models/model.py:571\u001b[0m, in \u001b[0;36mmodel_from_json\u001b[0;34m(json_string, custom_objects)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m serialization_lib\n\u001b[1;32m    570\u001b[0m model_config \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(json_string)\n\u001b[0;32m--> 571\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mserialization_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeserialize_keras_object\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/source/dls_2sem_nlp/.venv/lib/python3.10/site-packages/keras/src/saving/serialization_lib.py:687\u001b[0m, in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(config, custom_objects, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    684\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    685\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[0;32m--> 687\u001b[0m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43m_retrieve_class_or_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m    \u001b[49m\u001b[43mregistered_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclass\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfull_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mcls\u001b[39m, types\u001b[38;5;241m.\u001b[39mFunctionType):\n\u001b[1;32m    697\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\n",
      "File \u001b[0;32m~/source/dls_2sem_nlp/.venv/lib/python3.10/site-packages/keras/src/saving/serialization_lib.py:805\u001b[0m, in \u001b[0;36m_retrieve_class_or_fn\u001b[0;34m(name, registered_name, module, obj_type, full_config, custom_objects)\u001b[0m\n\u001b[1;32m    802\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    803\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[0;32m--> 805\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    806\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not locate \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobj_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    807\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure custom classes are decorated with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    808\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`@keras.saving.register_keras_serializable()`. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFull object config: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfull_config\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    810\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: Could not locate class 'Functional'. Make sure custom classes are decorated with `@keras.saving.register_keras_serializable()`. Full object config: {'class_name': 'Functional', 'config': {'name': 'model_1', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': [None, None, 56], 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'grammemes'}, 'name': 'grammemes', 'inbound_nodes': []}, {'class_name': 'Dropout', 'config': {'name': 'dropout_1', 'trainable': True, 'dtype': 'float32', 'rate': 0.2, 'noise_shape': None, 'seed': None}, 'name': 'dropout_1', 'inbound_nodes': [[['grammemes', 0, 0, {}]]]}, {'class_name': 'InputLayer', 'config': {'batch_input_shape': [None, None, 32], 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'chars'}, 'name': 'chars', 'inbound_nodes': []}, {'class_name': 'Dense', 'config': {'name': 'dense_1', 'trainable': True, 'dtype': 'float32', 'units': 30, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'name': 'dense_1', 'inbound_nodes': [[['dropout_1', 0, 0, {}]]]}, {'class_name': 'TimeDistributed', 'config': {'name': 'time_distributed_1', 'trainable': True, 'dtype': 'float32', 'layer': {'class_name': 'Functional', 'config': {'name': 'model_1', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': [None, 32], 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'chars'}, 'name': 'chars', 'inbound_nodes': []}, {'class_name': 'Embedding', 'config': {'name': 'chars_embeddings', 'trainable': True, 'batch_input_shape': [None, None], 'dtype': 'float32', 'input_dim': 101, 'output_dim': 24, 'embeddings_initializer': {'class_name': 'RandomUniform', 'config': {'minval': -0.05, 'maxval': 0.05, 'seed': None}}, 'embeddings_regularizer': None, 'activity_regularizer': None, 'embeddings_constraint': None, 'mask_zero': False, 'input_length': None}, 'name': 'chars_embeddings', 'inbound_nodes': [[['chars', 0, 0, {}]]]}, {'class_name': 'Reshape', 'config': {'name': 'reshape_1', 'trainable': True, 'dtype': 'float32', 'target_shape': [768]}, 'name': 'reshape_1', 'inbound_nodes': [[['chars_embeddings', 0, 0, {}]]]}, {'class_name': 'Dropout', 'config': {'name': 'dropout_2', 'trainable': True, 'dtype': 'float32', 'rate': 0.2, 'noise_shape': None, 'seed': None}, 'name': 'dropout_2', 'inbound_nodes': [[['reshape_1', 0, 0, {}]]]}, {'class_name': 'Dense', 'config': {'name': 'dense_2', 'trainable': True, 'dtype': 'float32', 'units': 500, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'name': 'dense_2', 'inbound_nodes': [[['dropout_2', 0, 0, {}]]]}, {'class_name': 'Dropout', 'config': {'name': 'dropout_3', 'trainable': True, 'dtype': 'float32', 'rate': 0.2, 'noise_shape': None, 'seed': None}, 'name': 'dropout_3', 'inbound_nodes': [[['dense_2', 0, 0, {}]]]}, {'class_name': 'Dense', 'config': {'name': 'dense_3', 'trainable': True, 'dtype': 'float32', 'units': 200, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'name': 'dense_3', 'inbound_nodes': [[['dropout_3', 0, 0, {}]]]}, {'class_name': 'Dropout', 'config': {'name': 'dropout_4', 'trainable': True, 'dtype': 'float32', 'rate': 0.2, 'noise_shape': None, 'seed': None}, 'name': 'dropout_4', 'inbound_nodes': [[['dense_3', 0, 0, {}]]]}], 'input_layers': [['chars', 0, 0]], 'output_layers': [['dropout_4', 0, 0]]}}}, 'name': 'time_distributed_1', 'inbound_nodes': [[['chars', 0, 0, {}]]]}, {'class_name': 'Concatenate', 'config': {'name': 'LSTM_input', 'trainable': True, 'dtype': 'float32', 'axis': -1}, 'name': 'LSTM_input', 'inbound_nodes': [[['dense_1', 0, 0, {}], ['time_distributed_1', 0, 0, {}]]]}, {'class_name': 'Dense', 'config': {'name': 'dense_4', 'trainable': True, 'dtype': 'float32', 'units': 200, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'name': 'dense_4', 'inbound_nodes': [[['LSTM_input', 0, 0, {}]]]}, {'class_name': 'LSTM', 'config': {'name': 'LSTM_1_forward', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': False, 'stateful': False, 'unroll': False, 'time_major': False, 'units': 128, 'activation': 'tanh', 'recurrent_activation': 'hard_sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None}, 'shared_object_id': 25}, 'recurrent_initializer': {'class_name': 'Orthogonal', 'config': {'gain': 1.0, 'seed': None}, 'shared_object_id': 26}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}, 'shared_object_id': 27}, 'unit_forget_bias': True, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.3, 'recurrent_dropout': 0.3, 'implementation': 1}, 'name': 'LSTM_1_forward', 'inbound_nodes': [[['dense_4', 0, 0, {}]]]}, {'class_name': 'ReversedLSTM', 'config': {'name': 'LSTM_1_backward', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': True, 'stateful': False, 'unroll': False, 'time_major': False, 'units': 128, 'activation': 'tanh', 'recurrent_activation': 'hard_sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None}, 'shared_object_id': 30}, 'recurrent_initializer': {'class_name': 'Orthogonal', 'config': {'gain': 1.0, 'seed': None}, 'shared_object_id': 31}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}, 'shared_object_id': 32}, 'unit_forget_bias': True, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.3, 'recurrent_dropout': 0.3, 'implementation': 1}, 'name': 'LSTM_1_backward', 'inbound_nodes': [[['dense_4', 0, 0, {}]]]}, {'class_name': 'Concatenate', 'config': {'name': 'BiLSTM_input', 'trainable': True, 'dtype': 'float32', 'axis': -1}, 'name': 'BiLSTM_input', 'inbound_nodes': [[['LSTM_1_forward', 0, 0, {}], ['LSTM_1_backward', 0, 0, {}]]]}, {'class_name': 'Bidirectional', 'config': {'name': 'bidirectional_1', 'trainable': True, 'dtype': 'float32', 'layer': {'class_name': 'LSTM', 'config': {'name': 'LSTM_0', 'trainable': True, 'dtype': 'float32', 'return_sequences': True, 'return_state': False, 'go_backwards': False, 'stateful': False, 'unroll': False, 'time_major': False, 'units': 128, 'activation': 'tanh', 'recurrent_activation': 'hard_sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None}, 'shared_object_id': 36}, 'recurrent_initializer': {'class_name': 'Orthogonal', 'config': {'gain': 1.0, 'seed': None}, 'shared_object_id': 37}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}, 'shared_object_id': 38}, 'unit_forget_bias': True, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.3, 'recurrent_dropout': 0.3, 'implementation': 1}}, 'merge_mode': 'concat'}, 'name': 'bidirectional_1', 'inbound_nodes': [[['BiLSTM_input', 0, 0, {}]]]}, {'class_name': 'TimeDistributed', 'config': {'name': 'time_distributed_2', 'trainable': True, 'dtype': 'float32', 'layer': {'class_name': 'Dense', 'config': {'name': 'dense_5', 'trainable': True, 'dtype': 'float32', 'units': 128, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}}, 'name': 'time_distributed_2', 'inbound_nodes': [[['bidirectional_1', 0, 0, {}]]]}, {'class_name': 'TimeDistributed', 'config': {'name': 'time_distributed_3', 'trainable': True, 'dtype': 'float32', 'layer': {'class_name': 'Dropout', 'config': {'name': 'dropout_2', 'trainable': True, 'dtype': 'float32', 'rate': 0.2, 'noise_shape': None, 'seed': None}}}, 'name': 'time_distributed_3', 'inbound_nodes': [[['time_distributed_2', 0, 0, {}]]]}, {'class_name': 'TimeDistributed', 'config': {'name': 'time_distributed_4', 'trainable': True, 'dtype': 'float32', 'layer': {'class_name': 'BatchNormalization', 'config': {'name': 'batch_normalization_1', 'trainable': True, 'dtype': 'float32', 'axis': [1], 'momentum': 0.99, 'epsilon': 0.001, 'center': True, 'scale': True, 'beta_initializer': {'class_name': 'Zeros', 'config': {}}, 'gamma_initializer': {'class_name': 'Ones', 'config': {}}, 'moving_mean_initializer': {'class_name': 'Zeros', 'config': {}}, 'moving_variance_initializer': {'class_name': 'Ones', 'config': {}}, 'beta_regularizer': None, 'gamma_regularizer': None, 'beta_constraint': None, 'gamma_constraint': None}}}, 'name': 'time_distributed_4', 'inbound_nodes': [[['time_distributed_3', 0, 0, {}]]]}, {'class_name': 'TimeDistributed', 'config': {'name': 'time_distributed_5', 'trainable': True, 'dtype': 'float32', 'layer': {'class_name': 'Activation', 'config': {'name': 'activation_1', 'trainable': True, 'dtype': 'float32', 'activation': 'relu'}}}, 'name': 'time_distributed_5', 'inbound_nodes': [[['time_distributed_4', 0, 0, {}]]]}, {'class_name': 'Dense', 'config': {'name': 'main_pred', 'trainable': True, 'dtype': 'float32', 'units': 253, 'activation': 'softmax', 'use_bias': True, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'name': 'main_pred', 'inbound_nodes': [[['time_distributed_5', 0, 0, {}]]]}], 'input_layers': [['grammemes', 0, 0], ['chars', 0, 0]], 'output_layers': [['main_pred', 0, 0]]}, 'keras_version': '2.8.0', 'backend': 'tensorflow'}"
     ]
    }
   ],
   "source": [
    "# 3.\n",
    "predictor = RNNMorphPredictor(language=\"ru\")\n",
    "rnnmorph_result = predictor.predict(text_ru.split(' '))\n",
    "[(token.normal_form, token.posm, token.tag) for token in rnnmorph_result[:7]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named entities recognition (Поиск именованых сущностей)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple 0 5 ORG\n",
      "U.K. 26 30 GPE\n",
      "$1 bullion 43 53 MONEY\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Apple is looking at buing U.K. startup for $1 bullion\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text,\n",
    "          ent.start_char,\n",
    "          ent.end_char,\n",
    "          ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Численное представление текста"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Смотрим набор данных fetch_20newsgroups в задаче классификации темы по новости"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.utils._bunch.Bunch'>\n"
     ]
    }
   ],
   "source": [
    "newsgroups_train = fetch_20newsgroups(subset=\"train\")\n",
    "print(type(newsgroups_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Количество новостей\n",
    "newsgroups_train.filenames.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_train.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Рассмотрим подвыборку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2034,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories = [\"alt.atheism\", \"talk.religion.misc\",\n",
    "              \"comp.graphics\", \"sci.space\"]\n",
    "newsgroups_train = fetch_20newsgroups(subset=\"train\", categories=categories)\n",
    "newsgroups_train.filenames.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> From: rych@festival.ed.ac.uk (R Hawkes)\n",
      "Subject: 3DS: Where did all the texture rules go?\n",
      "Lines: 21\n",
      "\n",
      "Hi,\n",
      "\n",
      "I've noticed that if you only save a model (with all your mapping planes\n",
      "positioned carefully) to a .3DS file that when you reload it after restarting\n",
      "3DS, they are given a default position and orientation.  But if you save\n",
      "to a .PRJ file their positions/orientation are preserved.  Does anyone\n",
      "know why this information is not stored in the .3DS file?  Nothing is\n",
      "explicitly said in the manual about saving texture rules in the .PRJ file. \n",
      "I'd like to be able to read the texture rule information, does anyone have \n",
      "the format for the .PRJ file?\n",
      "\n",
      "Is the .CEL file format available from somewhere?\n",
      "\n",
      "Rych\n",
      "\n",
      "======================================================================\n",
      "Rycharde Hawkes\t\t\t\temail: rych@festival.ed.ac.uk\n",
      "Virtual Environment Laboratory\n",
      "Dept. of Psychology\t\t\tTel  : +44 31 650 3426\n",
      "Univ. of Edinburgh\t\t\tFax  : +44 31 667 0150\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(type(newsgroups_train.data), newsgroups_train.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Первые 10 меток"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 3, 2, 0, 2, 0, 2, 1, 2, 1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_train.target[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Векторизация текстов с помощью TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Некоторые параметры:  \n",
    "- input: str ('filename', 'file', 'content')\n",
    "- lowercase: bool, default True\n",
    "- preprocessor: callable, default None\n",
    "- stop_words: str ('english'), list, default None\n",
    "- ngram_range: tuple (min_n, max_n)\n",
    "- max_df: float in range [0., 1.] or int, default 1.\n",
    "- min_df: floaf in range [0., 1.] or int, default 1\n",
    "- max_features: int, default None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2034, 42307)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(lowercase=False)\n",
    "vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
    "vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2034, 34118)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lowercase\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
    "vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Посмотрим некоторые слова, встречающиеся в тексте"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['00', '000', '0000', '00000', '000000', '000005102000', '000021',\n",
       "       '000062david42', '0000vec', '0001'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names_out()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Посмотрим на слова, которые встречаются в 80% случаях"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2034, 9)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=0.8)\n",
    "vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
    "vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Посмотрим на самые часто встречающиеся слова\n",
    "Как мы видим, у нас есть много слов, типа from, lines, the, to, которые не несут в себе смысловой информации. Мы хотим от них избавиться"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['and', 'from', 'in', 'lines', 'of', 'organization', 'subject',\n",
       "       'the', 'to'], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Посмотрим на то, сколько у нас будет слов, если мы будем работать со словами, встречающиеся в диапазоне от 1% до 80%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2034, 2391)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=0.01, max_df=0.8)\n",
    "vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
    "vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Рассмотрим ngram-range для отлавливания взаимодействия между токенами, а не токены по отдельности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2034, 1236)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1, 3), min_df=0.03, max_df=0.9)\n",
    "vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
    "vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Используем Стоп-слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/vitalii/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'oh , think landed miracle work , thirst hunger come conference bird'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Стоп-слова и preprocessor\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "nltk.download(\"wordnet\")\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "\n",
    "def preproc_nltk(text):\n",
    "    # text = re.sub(f\"[{string.punctuation}]\", ' ', text)\n",
    "    return ' '.join([wnl.lemmatize(word) for word in word_tokenize(text.lower()) if word not in stop_words])\n",
    "\n",
    "st = \"Oh, I think ve landed where there are miracles at work, For the thirst and for the hunger Come the conference of birds\"\n",
    "preproc_nltk(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.02 s, sys: 0 ns, total: 7.02 s\n",
      "Wall time: 7.03 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vectorizer = TfidfVectorizer(preprocessor=preproc_nltk)\n",
    "vectors = vectorizer.fit_transform(newsgroups_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'oh , I think land miracle work , thirst hunger come conference bird'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preprocessor_spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "texts = newsgroups_train.data.copy()\n",
    "\n",
    "def preproc_spacy(text):\n",
    "    spacy_results = nlp(text)\n",
    "    return ' '.join([token.lemma_ for token in spacy_results if token.lemma_ not in stop_words])\n",
    "\n",
    "preproc_spacy(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.23 s, sys: 385 ms, total: 8.62 s\n",
      "Wall time: 29.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "new_texts = []\n",
    "for doc in nlp.pipe(texts, batch_size=32, n_process=3, disable=[\"parser\", \"ner\"]):\n",
    "    new_texts.append(' '.join([tok.lemma_ for tok in doc if tok.lemma not in stop_words]))\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectors = vectorizer.fit_transform(new_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: rych@festival.ed.ac.uk (R Hawkes)\n",
      "Subject: 3DS: Where did all the texture rules go?\n",
      "Lines: 21\n",
      "\n",
      "Hi,\n",
      "\n",
      "I've noticed that if you only save a model (with all your mapping planes\n",
      "positioned carefully) to a .3DS file that when you reload it after restarting\n",
      "3DS, they are given a default position and orientation.  But if you save\n",
      "to a .PRJ file their positions/orientation are preserved.  Does anyone\n",
      "know why this information is not stored in the .3DS file?  Nothing is\n",
      "explicitly said in the manual about saving texture rules in the .PRJ file. \n",
      "I'd like to be able to read the texture rule information, does anyone have \n",
      "the format for the .PRJ file?\n",
      "\n",
      "Is the .CEL file format available from somewhere?\n",
      "\n",
      "Rych\n",
      "\n",
      "======================================================================\n",
      "Rycharde Hawkes\t\t\t\temail: rych@festival.ed.ac.uk\n",
      "Virtual Environment Laboratory\n",
      "Dept. of Psychology\t\t\tTel  : +44 31 650 3426\n",
      "Univ. of Edinburgh\t\t\tFax  : +44 31 667 0150\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(newsgroups_train.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from : rych@festival.ed.ac.uk ( R Hawkes ) \n",
      " subject : 3ds : where do all the texture rule go ? \n",
      " line : 21 \n",
      "\n",
      " Hi , \n",
      "\n",
      " I have notice that if you only save a model ( with all your mapping plane \n",
      " position carefully ) to a .3ds file that when you reload it after restart \n",
      " 3ds , they be give a default position and orientation .   but if you save \n",
      " to a .prj file their position / orientation be preserve .   do anyone \n",
      " know why this information be not store in the .3ds file ?   nothing be \n",
      " explicitly say in the manual about save texture rule in the .prj file . \n",
      " I would like to be able to read the texture rule information , do anyone have \n",
      " the format for the .PRJ file ? \n",
      "\n",
      " be the .cel file format available from somewhere ? \n",
      "\n",
      " rych \n",
      "\n",
      " = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = \n",
      " Rycharde Hawkes \t\t\t\t email : rych@festival.ed.ac.uk \n",
      " Virtual Environment Laboratory \n",
      " Dept . of psychology \t\t\t Tel   : +44 31 650 3426 \n",
      " Univ . of Edinburgh \t\t\t fax   : +44 31 667 0150 \n",
      " = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(new_texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Итоговая модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['000', 'au', 'christ', 'engineering', 'human', 'look', 'of this',\n",
       "       'report', 'technology', 'understand'], dtype=object)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1, 3), max_df=0.5, max_features=1000)\n",
    "vectors = vectorizer.fit_transform(new_texts)\n",
    "vectorizer.get_feature_names_out()[::100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2034, 1000)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['zoo toronto edu', 'under', 'technical', 'reply to', 'of these',\n",
       "       'long', 'however', 'end', 'choose', 'attempt'], dtype=object)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names_out()[::-100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['000', 'au', 'christ', 'engineering', 'human', 'look', 'of this',\n",
       "       'report', 'technology', 'understand'], dtype=object)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names_out()[::100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Посмотрим на косинусную меру между словами\n",
    "- Косинусное расстояние - это 1 - косинусная схожесть (чем меньше, тем лучше)\n",
    "- Косинусная мера близость (схожесть) (чем больше, тем лучше)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse._csr.csr_matrix"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector = vectors.todense()[0]\n",
    "(vector != 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2034, 1000)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense_vectors = vectors.todense()\n",
    "dense_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(v1, v2):\n",
    "    # v1, v2 (1 x dim)\n",
    "    return np.array(v1 @ v2.T / norm(v1) / norm(v2))[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0000000000000002"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_sim(dense_vectors[0], dense_vectors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosines = []\n",
    "for i in range(10):\n",
    "    cosines.append(cosine_sim(dense_vectors[0], dense_vectors[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Схожесть между 0 и первыми 10-ю новостями"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0000000000000002,\n",
       " 0.04191279776414236,\n",
       " 0.005868383611019931,\n",
       " 0.09771238093526102,\n",
       " 0.07060916453270281,\n",
       " 0.06745764842966309,\n",
       " 0.0267141823627476,\n",
       " 0.22853760897260958,\n",
       " 0.031636420124663965,\n",
       " 0.06928662593161493]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Классы для первых 10 новостей: [1, 3, 2, 0, 2, 0, 2, 1, 2, 1]\n",
    "cosines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Обучим любую известную модель на полученных признаках"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1627,), (407,))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(dense_vectors,\n",
    "                                                    newsgroups_train.target,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=0)\n",
    "\n",
    "y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.08 s, sys: 437 µs, total: 1.08 s\n",
      "Wall time: 1.07 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;SVC<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.svm.SVC.html\">?<span>Documentation for SVC</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>SVC()</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "svc = svm.SVC()\n",
    "X_train = np.asarray(X_train)\n",
    "svc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9238329238329238"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = np.asarray(X_test)\n",
    "accuracy_score(y_test, svc.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9164619164619164"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd = SGDClassifier()\n",
    "sgd.fit(X_train, y_train)\n",
    "accuracy_score(y_test, sgd.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Embeddings\n",
    "Качество для эмбеддингов хуже, чем для TF-IDF обработки. Дело в том, что мы использовали малый размер для эмбеддингов (25, 32 по сравнению с 1000)  \n",
    "Достаточный размер эмбеддинга должен быть около 200-300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_pretrained = api.load(\"glove-twitter-25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<user>',\n",
       " '.',\n",
       " ':',\n",
       " 'rt',\n",
       " ',',\n",
       " '<repeat>',\n",
       " '<hashtag>',\n",
       " '<number>',\n",
       " '<url>',\n",
       " '!']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_pretrained.index_to_key[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1193514\n"
     ]
    }
   ],
   "source": [
    "print(len(embeddings_pretrained.index_to_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_words = [preproc_nltk(text).split() for text in newsgroups_train.data]\n",
    "embeddings_trained = Word2Vec(proc_words,  # data for model to train on\n",
    "                              vector_size=32,  # embedding vector size\n",
    "                              min_count=3,  # consider words that occured at least 5 times\n",
    "                              window=3).wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_sum(comment, embeddings):\n",
    "    \"\"\"\n",
    "    Implement a function that converts preprocessed comment to a sum of token vectors\n",
    "    \"\"\"\n",
    "    embedding_dim = embeddings.vectors.shape[1]\n",
    "    features = np.zeros([embedding_dim], dtype=\"float32\")\n",
    "\n",
    "    for word in preproc_nltk(comment).split():\n",
    "        if word in embeddings:\n",
    "            features += embeddings[f'{word}']\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1627, 25), (407, 25))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_wv = np.stack([vectorize_sum(text, embeddings_pretrained) for text in newsgroups_train.data])\n",
    "X_train_wv, X_test_wv, y_train, y_test = train_test_split(X_wv, newsgroups_train.target, test_size=0.2, random_state=0)\n",
    "X_train_wv.shape, X_test_wv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vitalii/source/dls_2sem_nlp/.venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7027027027027027"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(max_iter=5000)\n",
    "wv_model = clf.fit(X_train_wv, y_train)\n",
    "accuracy_score(y_test, wv_model.predict(X_test_wv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1627, 32), (407, 32))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_wv = np.stack([vectorize_sum(text, embeddings_trained) for text in newsgroups_train.data])\n",
    "X_train_wv, X_test_wv, y_train, y_test = train_test_split(X_wv, newsgroups_train.target, test_size=0.2, random_state=0)\n",
    "X_train_wv.shape, X_test_wv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13566"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings_trained.index_to_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Качество для обученных эмбеддингов на своём тексте выше, чем для предобученных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vitalii/source/dls_2sem_nlp/.venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7321867321867321"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(max_iter=5000)\n",
    "wv_model = clf.fit(X_train_wv, y_train)\n",
    "accuracy_score(y_test, wv_model.predict(X_test_wv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Используем более длинные эмбеддинги"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vitalii/source/dls_2sem_nlp/.venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8476658476658476"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proc_words = [preproc_nltk(text).split() for text in newsgroups_train.data]\n",
    "embeddings_trained = Word2Vec(proc_words,  # data for model to train on\n",
    "                              vector_size=200,  # embedding vector size\n",
    "                              min_count=3,  # consider words that occured at least 5 times\n",
    "                              window=3).wv\n",
    "\n",
    "X_wv = np.stack([vectorize_sum(text, embeddings_trained) for text in newsgroups_train.data])\n",
    "X_train_wv, X_test_wv, y_train, y_test = train_test_split(X_wv, newsgroups_train.target, test_size=0.2, random_state=0)\n",
    "X_train_wv.shape, X_test_wv.shape\n",
    "\n",
    "clf = LogisticRegression(max_iter=5000)\n",
    "wv_model = clf.fit(X_train_wv, y_train)\n",
    "accuracy_score(y_test, wv_model.predict(X_test_wv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Увеличим количество итераций для логистической регрессии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vitalii/source/dls_2sem_nlp/.venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8427518427518428"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(max_iter=10000)\n",
    "wv_model = clf.fit(X_train_wv, y_train)\n",
    "accuracy_score(y_test, wv_model.predict(X_test_wv))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
